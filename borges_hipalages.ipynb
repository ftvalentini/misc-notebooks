{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "borges_hipalages.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ftvalentini/misc-notebooks/blob/master/borges_hipalages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsDGAS4lSigV",
        "colab_type": "text"
      },
      "source": [
        "# Buscando hipálages en los cuentos de Borges con NLP\n",
        "#### Autores: Julián Martinez Correa y Francisco Valentini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFRHqPQuV6V5",
        "colab_type": "text"
      },
      "source": [
        "En Twitter alguién sugirió recopilar todas las hipálages de la literatura de Borges y [@manuelaristaran sugirió usar alguna técnica de Natural Language Processing](https://twitter.com/manuelaristaran/status/1249775645929934851) para esta tarea.  \n",
        "Una hipálage es una combinación de palabras en la que a un sustantivo se le atribuye una acción (verbo) o una cualidad (adjetivo) que son propios de otro. Por ejemplo, la expresión \"noche unámine\" en Las Ruinas Circulares.  \n",
        "Siguiendo la sugerencia, decidimos abordar de forma exploratoria esta tarea con los siguientes pasos:  \n",
        "- Restringimos el análisis a pares de palabras sustantivo-adjetivo y a [un compilado de ficciones de Borges **en inglés**](https://posthegemony.files.wordpress.com/2013/02/borges_collected-fictions.pdf)\n",
        "- Extraemos todos los pares sustantivo-adjetivo de los cuentos usando un [modelo de dependecy parsing pre-entrenado](https://stanfordnlp.github.io/stanza/depparse.html) que reconoce las dependencias gramáticales en cada oración\n",
        "- Medimos las distancias coseno entre los [word embeddings GloVe](https://nlp.stanford.edu/projects/glove/) de cada par de palabras. Un word embedding es un vector de dimensión *n* que codifica el signifcado de una palabra en relación al resto de las palabras de un vocabulario.\n",
        "- Rankeamos los pares según la distancia coseno y exploramos rankings alternativos en función de la inspección visual de los resultados. \n",
        "<!-- normaliza la distancia por la distancia promedio del sustantivo o adejtivo a los *n* adjetivos o sustantivos más cercanos. -->\n",
        "\n",
        "La hipótesis es que hay una mayor probabilidad de encontrar hipálages en pares sustantivo-adjetivo con distancia alta entre sus embeddings que en pares con distancia baja. Esto se debe a que **en general** las palabras que conforman las hipálages tienen una similitud semántica baja o al menos no tienden a usarse en el mismo contexto. \n",
        "\n",
        "En este sentido, este análisis no nos permite estricamente encontrar hipálages (y aún menos evaluar la bondad de los resultados), sino más bien encontrar pares de sustantivo-adjetivos inusuales, llamativos o interesantes en relación al uso común de la lengua inglesa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78G5BTgWY-Hi",
        "colab_type": "text"
      },
      "source": [
        "### Descargas y librerías"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbBI9NqVR6_b",
        "colab_type": "text"
      },
      "source": [
        "- Instalamos [Stanza](https://github.com/stanfordnlp/stanza/), la librería desarrollada por Stanford que vamos a usar para encontrar las dependencias gramaticales en los textos. La librería ofrece modelos pre-entrenados para múltiples idiomas para aplicar en el contexto de tareas de NLP (en nuestro caso, dependency parsing). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43HvACxkRymt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hOjNVOosyyD",
        "colab_type": "text"
      },
      "source": [
        "- Descargamos `PyPDF2` para convertir *Ficciones* de formato PDF a un archivo de texto plano.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UneCaUCbsyGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install PyPDF2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f24WUOPTrehv",
        "colab_type": "text"
      },
      "source": [
        "- Cargamos las librerías que vamos a necesitar para hacer el análisis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMXXBGtUrgY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import stanza\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import gensim.downloader as gensim_api\n",
        "\n",
        "import PyPDF2 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWRwt3cfrH9o",
        "colab_type": "text"
      },
      "source": [
        "- Descargamos los modelos de `stanza` para textos en inglés. Los modelos se guardan por defecto en `/root/stanza_resources`. Esta carga nos llevó poco tiempo porque usamos Google Colab,  pero en otro contexto puede llevar algunos minutos -- lo mismo vale para el resto de instalaciones o descargas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG-14viUrFxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "stanza.download('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU3cRBTTUm8U",
        "colab_type": "text"
      },
      "source": [
        "- Cargamos los vectores [GloVe](https://nlp.stanford.edu/pubs/glove.pdf), word embeddings ajustados por Stanford. En particular, vamos a usar los vectores de mayor dimensión posible (300) y ajustados con la wikipedia en inglés."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3xi5J58Ulyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_embeddings():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Returns: array of size (vocab_size, embeddings_dim)\n",
        "    \"\"\"\n",
        "    vectors = gensim_api.load(\"glove-wiki-gigaword-300\")\n",
        "    return vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mD_axisWcvY",
        "colab_type": "code",
        "outputId": "16de87cd-a8aa-41a6-c4f3-33edff789fdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%time\n",
        "word_vectors = load_embeddings()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==============================================----] 93.2% 350.5/376.1MB downloaded"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "602157AvS6wp",
        "colab_type": "text"
      },
      "source": [
        "- Descargamos los modelos de tokenización `punkt` para separar textos en oraciones. Solo lo vamos a usar para visualizar el texto de forma más amigable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l3TBrYtRGoP",
        "colab_type": "code",
        "outputId": "519e049a-900c-4d1b-f092-40ab8c15192e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Uj7VpRMca4g",
        "colab_type": "text"
      },
      "source": [
        "- Convertimos el pdf con los cuentos en un archivo de texto. Leemos este archivo y lo almacenamos en `texto` como una gran cadena de caracteres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tao4BqxXclFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pdf_to_txt(pdf_file, txt_file, pdf_first_page=10):\n",
        "    \"\"\" Convert pdf_file to txt_file with strings\n",
        "    \"\"\"\n",
        "    with open(pdf_file,'rb') as pdf, open(txt_file, 'w') as txt:\n",
        "        read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
        "        number_of_pages = read_pdf.getNumPages()\n",
        "        for page_number in range(pdf_first_page-1, number_of_pages):  \n",
        "            page = read_pdf.getPage(page_number)\n",
        "            page_content = page.extractText()\n",
        "            txt.write(page_content)\n",
        "\n",
        "def read_text(file):\n",
        "    \"\"\" Reads file as string\n",
        "    file can be PDF or txt -- if it is PDF it first transforms to txt\n",
        "    \"\"\"\n",
        "    file_root = os.path.splitext(file)[0]   \n",
        "    file_ext = os.path.splitext(file)[1] \n",
        "    if file_ext == '.pdf':\n",
        "        pdf_to_txt(file, file_root+'.txt')\n",
        "    with open(file_root+'.txt', \"r\", encoding='utf-8') as f:\n",
        "        texto = f.read() \n",
        "    return texto"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ldCOx3Zs-FB",
        "colab_type": "code",
        "outputId": "60d07feb-b453-416d-b0d5-925901e90684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "input_file = 'borges_collected-fictions.pdf'\n",
        "# input_file = 'the_library_of_babel.txt'\n",
        "texto = read_text(input_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 7.61 s, sys: 19.1 ms, total: 7.63 s\n",
            "Wall time: 7.64 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL4vFx33fvlT",
        "colab_type": "text"
      },
      "source": [
        "### Limpieza del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFDGTupPf2AK",
        "colab_type": "text"
      },
      "source": [
        "Limpiamos el texto de la siguiente manera:  \n",
        "- Reemplazamos los saltos de linea y cualquier otro carácter de tipo whitespace por espacios no repetidos porque no nos interesan los cambios de párrafo ni de página, sino solamente las oraciones  \n",
        "- Reemplazamos el carácter Š por --\n",
        "\n",
        "La documentación de `stanza` sugiere juntar varias oraciones en batches separando cada oración con dos saltos de linea `\\n\\n` para acelerar el procesamiento de muchos documentos. Sin embargo esto dificultaría rastrear al final del análisis las oraciones donde se encuentran las potenciales hipálages -- decidimos entonces conservar el texto como un solo string y que `stanza` se encargue de la tokenización.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Tz1tKKfybt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_texto(texto):\n",
        "    \"\"\"Cleans raw text\n",
        "       Returns: clean string\n",
        "    \"\"\"\n",
        "    clean_text = \" \".join(texto.split())\n",
        "    clean_text = clean_text.replace(\"Š\",\"—\")\n",
        "    return clean_text\n",
        "\n",
        "texto = clean_texto(texto)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqCbHS-vtsE7",
        "colab_type": "text"
      },
      "source": [
        "Veamos los primeros 500 caracteres del texto separados en oraciones para facilitar la visualización:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6Ot5QJvtvAM",
        "colab_type": "code",
        "outputId": "568b03f6-031c-4867-fd88-ae58e68bd09f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "for s in sent_tokenize(texto[:500]):\n",
        "    print(s)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A Universal History of Iniquity (1935) I inscribe this book to S.D.\n",
            "— English, innumerable, and an Angel.\n",
            "Also: I offer her that kernel of myself that I have saved, somehow— the central heart that deals not in words, traffics not with dreams, and is untouched by time, by joy, by adversities.\n",
            "Preface to the First Edition The exercises in narrative prose that constitute this book were performed from 1933 to 1934.\n",
            "They are derived, I think, from my re-readings of Stevenson and Chesterton, from the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv3vYyauaApY",
        "colab_type": "text"
      },
      "source": [
        "### Dependencies parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gIezB-xaFCS",
        "colab_type": "text"
      },
      "source": [
        "Para realizar tareas de NLP con `stanza` es necesario construir un Pipeline con procesadores acordes al idioma de análisis.\n",
        "El Pipeline se inicializa por defecto con un dependency parser (`depparse`) para determinar las dependencias sintácticas entre las palabras de una oración dada. Para poder correr este modelo es necesario procesar el texto de antemano con los siguientes procesadores:\n",
        "- `tokenize`: separa el documento en oraciones\n",
        "- `pos`: identifica el rol de cada palabra en la oración con un modelo de Part-of-Speech (POS) tagging pre-entrenado\n",
        "- `lemma`: halla el lema correspondiente de cada palabra (la forma que representa las posibles formas flexionadas -- por ejemplo, \"walk\" es el lema de \"walking\")\n",
        "\n",
        "Por defecto estos procesadores se ejecutan antes de `depparse`, el cual usa la información generada en los pasos precedentes para identificar las relaciones sintátictas. Sugerimos revisar [la documentación de `stanza`](https://stanfordnlp.github.io/stanza/pipeline.html) para conocer más detalles. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-3vM_91aHFe",
        "colab_type": "code",
        "outputId": "cdcd0866-a753-4ba7-cfa2-fc2cdc352510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "nlp = stanza.Pipeline('en', verbose=False)\n",
        "docs = nlp(texto)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 40min 39s, sys: 27.3 s, total: 41min 6s\n",
            "Wall time: 41min 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYqXKSuOVB4i",
        "colab_type": "text"
      },
      "source": [
        "Para identificar las relaciones sustantivo-adjetivo es necesario conservar las relaciones de tipo \"amod\" (*adjectival modifier*). Para cada relación identificada conservamos el índice de la oración para poder rastrearla más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjMLY_v4T2D0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_dependencies(doc):\n",
        "    \"\"\"Find amod dependencies in one parsed doc with one or more sentences \n",
        "    Returns: list of dependencies with (sentence_idx, head, dependent)\n",
        "    \"\"\"\n",
        "    deps = list()\n",
        "    for sent_idx, sent in enumerate(doc.sentences):\n",
        "        id2word = {word.id: word.text for word in sent.words}\n",
        "        deps += [(sent_idx, id2word[str(word.head)], word.text) \\\n",
        "                for word in sent.words if word.deprel=='amod']\n",
        "    return deps\n",
        "    \n",
        "deps = find_dependencies(docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGKO8R24VXYr",
        "colab_type": "text"
      },
      "source": [
        "Veamos los tres primeros y los tres últimos pares hallados: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxrFcYe5VhEd",
        "colab_type": "code",
        "outputId": "7479d11e-37e1-4f82-d285-f317d0fef55c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(deps[:3])\n",
        "print(deps[-3:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(2, 'heart', 'central', 0.734546571969986), (4, 'prose', 'narrative', 0.3734986186027527), (5, 'films', 'first', 0.7076624929904938)]\n",
            "[(10788, 'layers', 'Everlasting'), (10793, 'aesthetics', 'literary', 0.6261562705039978), (10793, 'others', 'many', 0.2628936767578125)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KL0xmf55A6ra"
      },
      "source": [
        "## Word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1mp5kzDAZaE",
        "colab_type": "text"
      },
      "source": [
        "Para cada par de palabras calculamos la distancia entre sus embeddings. Si no existe el word embedding para una palabra, no calculamos la distancia. Finalmente guardamos los resultados en un DataFrame. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K74AdSr1xJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# palabras disponibles en GloVe\n",
        "vocab = word_vectors.vocab.keys()\n",
        "print(len(vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEg_y5RKZZVD",
        "colab_type": "code",
        "outputId": "ce3775f1-d4e6-433c-9b54-1a225270d5d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "%%time\n",
        "for i, dep in enumerate(deps):\n",
        "    _words = dep[1:] \n",
        "    if set(_words).issubset(vocab):\n",
        "        deps[i] = dep + (word_vectors.distance(*_words),)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`. [matutils.py:737]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11min 50s, sys: 2.76 s, total: 11min 53s\n",
            "Wall time: 11min 53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcvA4iaUmh9n",
        "colab_type": "code",
        "outputId": "1fc1d1e2-bc3e-478c-ca5c-4f43a64e359d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "deps_df = pd.DataFrame(deps, columns=['sent_idx', 'noun', 'adj', 'dist']) \n",
        "deps_df.sort_values(by=['dist'], inplace=True, ascending=False)\n",
        "deps_df.dropna(inplace=True)\n",
        "print(deps_df.shape)\n",
        "print(deps_df.head(20))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10087, 4)\n",
            "      sent_idx          noun           adj      dist\n",
            "4758      3707   schismatics           new  1.307657\n",
            "9150      8523  trivialities           new  1.283182\n",
            "898        629     anathemas         first  1.278724\n",
            "6918      5649      hecatomb        public  1.271595\n",
            "6231      5087        karats          good  1.262450\n",
            "4615      3576       paisano          same  1.247534\n",
            "6212      5072  alexandrines          long  1.247089\n",
            "3949      3017       impiety          near  1.246878\n",
            "1731      1333       milonga          same  1.242564\n",
            "7118      5864           men   punctilious  1.235335\n",
            "9740      9394        series  misfortunate  1.232602\n",
            "3810      2864          past    modifiable  1.231535\n",
            "656        424        battle    indiscreet  1.231510\n",
            "6337      5186        center     ineffable  1.231363\n",
            "7123      5866     forewords          long  1.224867\n",
            "2694      1969      hexagons        native  1.221834\n",
            "1175       897    plastering        former  1.221735\n",
            "8908      8091        hatpin        little  1.220267\n",
            "705        466         house   hunchbacked  1.218015\n",
            "3199      2356        d'être          only  1.215836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onML2sZZJYTP",
        "colab_type": "code",
        "outputId": "b7572289-39f2-4865-bb08-4eae924eb42d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print(deps_df.tail(10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       sent_idx      noun   adj      dist\n",
            "7249       6016  everyone  else  0.187325\n",
            "7429       6407  everyone  else  0.187325\n",
            "8204       7228    anyone  else  0.185691\n",
            "5972       4893    anyone  else  0.185691\n",
            "6194       5068    anyone  else  0.185691\n",
            "1080        792  anything  else  0.167966\n",
            "6669       5463  somebody  else  0.165844\n",
            "10446     10059    nobody  else  0.156148\n",
            "7342       6225   anybody  else  0.154146\n",
            "6667       5456   anybody  else  0.154146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdVKomV_NNLI",
        "colab_type": "text"
      },
      "source": [
        "Por inspección visual notamos un patrón común en varios pares de palabras con distancia alta: la combinación de un sustantivo relativamente raro (como *schismatics* o *anathemas*) y un adjetivo relativamente común (como *new* o *first*).  \n",
        "Una hipótesis es que estos pares tienen una distancia alta no neceseriamente por la distancia semántica entre sustantivo-adjetivo sino más bien por la rareza relativa del sustantivo, que se encuentra relativamente muy alejado del resto de las palabras.         \n",
        "Decidimos descartar estos casos del análisis eliminando los pares donde se combina un sustantivo poco frecuente con un adjetivo muy frecuente.  \n",
        "Para esto usamos el ranking de las palabras según frecuencia almacenado en `word_vectors.vocab` en el atributo `count`. Este indicador ordena a las palabras según la frecuencia observada durante el entrenamiento de GloVe, de modo que cuánto más común es la palabra, más alto es.       "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP-5sEWcLdD3",
        "colab_type": "code",
        "outputId": "5cddda1e-b419-4d15-c756-8394a5ed1db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(word_vectors.vocab['house'].count)\n",
        "print(word_vectors.vocab['schismatics'].count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "399834\n",
            "130001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2zIyQ_HIty3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ranking as is\n",
        "deps_df[\"noun_freq_rank\"] = deps_df[\"noun\"].apply(lambda x: word_vectors.vocab[x].count)\n",
        "deps_df[\"adj_freq_rank\"] = deps_df[\"adj\"].apply(lambda x: word_vectors.vocab[x].count)\n",
        "# ranking as percentile\n",
        "deps_df[\"noun_freq_rank_pct\"] = deps_df[\"noun_freq_rank\"].rank(pct=True)\n",
        "deps_df[\"adj_freq_rank_pct\"] =  deps_df[\"adj_freq_rank\"].rank(pct=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lozhgx-fAnTo",
        "colab_type": "text"
      },
      "source": [
        "Observamos los 10 sustantivos menos frecuentes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTEKoH1cMR8w",
        "colab_type": "code",
        "outputId": "29a86f32-5960-4d48-d17b-9c0c229870a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "deps_df[['noun','adj','noun_freq_rank_pct']].sort_values(by='noun_freq_rank_pct' \\\n",
        "    , ascending=True).head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>noun</th>\n",
              "      <th>adj</th>\n",
              "      <th>noun_freq_rank_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6213</th>\n",
              "      <td>alexandrines</td>\n",
              "      <td>formless</td>\n",
              "      <td>0.000149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6212</th>\n",
              "      <td>alexandrines</td>\n",
              "      <td>long</td>\n",
              "      <td>0.000149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7802</th>\n",
              "      <td>truco</td>\n",
              "      <td>handed</td>\n",
              "      <td>0.000297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9893</th>\n",
              "      <td>thingamajig</td>\n",
              "      <td>magical</td>\n",
              "      <td>0.000397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5155</th>\n",
              "      <td>effusiveness</td>\n",
              "      <td>exaggerated</td>\n",
              "      <td>0.000496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6231</th>\n",
              "      <td>karats</td>\n",
              "      <td>good</td>\n",
              "      <td>0.000595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6981</th>\n",
              "      <td>inabilities</td>\n",
              "      <td>phonetic</td>\n",
              "      <td>0.000744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6980</th>\n",
              "      <td>inabilities</td>\n",
              "      <td>own</td>\n",
              "      <td>0.000744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6918</th>\n",
              "      <td>hecatomb</td>\n",
              "      <td>public</td>\n",
              "      <td>0.000892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>cheroot</td>\n",
              "      <td>thoughtful</td>\n",
              "      <td>0.000991</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              noun          adj  noun_freq_rank_pct\n",
              "6213  alexandrines     formless            0.000149\n",
              "6212  alexandrines         long            0.000149\n",
              "7802         truco       handed            0.000297\n",
              "9893   thingamajig      magical            0.000397\n",
              "5155  effusiveness  exaggerated            0.000496\n",
              "6231        karats         good            0.000595\n",
              "6981   inabilities     phonetic            0.000744\n",
              "6980   inabilities          own            0.000744\n",
              "6918      hecatomb       public            0.000892\n",
              "199        cheroot   thoughtful            0.000991"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90FdBJpDAxc_",
        "colab_type": "text"
      },
      "source": [
        "...y los 10 adjetivos más frecuentes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOQvPOFlMg1W",
        "colab_type": "code",
        "outputId": "6e3f120d-fcff-4958-9bf6-2c14f8c4ab7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "deps_df[['noun','adj','adj_freq_rank_pct']].sort_values(by='adj_freq_rank_pct' \\\n",
        "    , ascending=False).head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>noun</th>\n",
              "      <th>adj</th>\n",
              "      <th>adj_freq_rank_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4758</th>\n",
              "      <td>schismatics</td>\n",
              "      <td>new</td>\n",
              "      <td>0.995787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10407</th>\n",
              "      <td>stories</td>\n",
              "      <td>new</td>\n",
              "      <td>0.995787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7009</th>\n",
              "      <td>education</td>\n",
              "      <td>new</td>\n",
              "      <td>0.995787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10547</th>\n",
              "      <td>government</td>\n",
              "      <td>new</td>\n",
              "      <td>0.995787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10644</th>\n",
              "      <td>government</td>\n",
              "      <td>new</td>\n",
              "      <td>0.995787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4130</th>\n",
              "      <td>man</td>\n",
              "      <td>new</td>\n",
              "      <td>0.995787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10897</th>\n",
              "      <td>economy</td>\n",
              "      <td>new</td>\n",
              "      <td>0.995787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10462</th>\n",
              "      <td>school</td>\n",
              "      <td>new</td>\n",
              "      <td>0.995787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6664</th>\n",
              "      <td>destiny</td>\n",
              "      <td>new</td>\n",
              "      <td>0.995787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2411</th>\n",
              "      <td>system</td>\n",
              "      <td>new</td>\n",
              "      <td>0.995787</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              noun  adj  adj_freq_rank_pct\n",
              "4758   schismatics  new           0.995787\n",
              "10407      stories  new           0.995787\n",
              "7009     education  new           0.995787\n",
              "10547   government  new           0.995787\n",
              "10644   government  new           0.995787\n",
              "4130           man  new           0.995787\n",
              "10897      economy  new           0.995787\n",
              "10462       school  new           0.995787\n",
              "6664       destiny  new           0.995787\n",
              "2411        system  new           0.995787"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzEmpRpbbG7e",
        "colab_type": "text"
      },
      "source": [
        "Finalmente descartamos un par de palabras dado si el adjetivo está entre el 5% de los más comunes y el sustantivo, entre el 5% de los más raros, tomando como total la cantidad de pares. Estos umbrales son complemetamente arbitrarios pero arrojan buenos resultados por inspección visual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVIP5WJKYDf8",
        "colab_type": "code",
        "outputId": "2384dd40-8636-4b90-f4eb-6723257e88ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "filtered_df = \\\n",
        "    deps_df.loc[(deps_df['noun_freq_rank_pct'] > 0.05) & (deps_df['adj_freq_rank_pct'] < 0.95)]\n",
        "print(filtered_df[['sent_idx','noun','adj','dist']].head(20))   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       sent_idx        noun           adj      dist\n",
            "7118       5864         men   punctilious  1.235335\n",
            "9740       9394      series  misfortunate  1.232602\n",
            "3810       2864        past    modifiable  1.231535\n",
            "656         424      battle    indiscreet  1.231510\n",
            "6337       5186      center     ineffable  1.231363\n",
            "2694       1969    hexagons        native  1.221834\n",
            "1175        897  plastering        former  1.221735\n",
            "705         466       house   hunchbacked  1.218015\n",
            "3199       2356      d'être          only  1.215836\n",
            "2464       1821     opinion  disconsolate  1.202806\n",
            "767         514     rustler          good  1.199905\n",
            "2151       1602       books       calmest  1.189988\n",
            "11394     10759     gauchos     following  1.189452\n",
            "428         280     request      pitiable  1.189281\n",
            "6398       5225      friend    lamentable  1.187334\n",
            "436         283     terrors        former  1.186086\n",
            "7500       6505        soul   sententious  1.183285\n",
            "9848       9591        rose     incarnate  1.183260\n",
            "6101       5015   entryways          long  1.182390\n",
            "2249       1663         aid       untried  1.181153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yrhtp8Oce1t",
        "colab_type": "text"
      },
      "source": [
        "Entre los pares de palabras que son potencialmente hipálages según el criterio de distancia vemos algunas muy interesantes. Podemos rastrearlas entre las oraciones originales usando el `sent_idx`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1zI8e7RZ9wk",
        "colab_type": "code",
        "outputId": "2f662fd0-8f17-4d86-f6b6-8a202781cd9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# hunchbacked house\n",
        "print(docs.sentences[466].text)\n",
        "# calmest books\n",
        "print(docs.sentences[1602].text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sometimes, from the garret window of some hunchbacked house near the water, a woman would dump a bucket of ashes onto the head of a passerby.\n",
            "This technique fills the calmest books with adventure.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1aYlzOa5tlC",
        "colab_type": "text"
      },
      "source": [
        "**FIN**"
      ]
    }
  ]
}