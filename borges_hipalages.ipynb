{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "borges_hipalages.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ftvalentini/misc-notebooks/blob/master/borges_hipalages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsDGAS4lSigV",
        "colab_type": "text"
      },
      "source": [
        "# Buscando hipálages en los cuentos de Borges con NLP\n",
        "#### Autores: Julián Martínez Correa y Francisco Valentini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFRHqPQuV6V5",
        "colab_type": "text"
      },
      "source": [
        "En Twitter alguién sugirió recopilar todas las hipálages de la literatura de Borges y [@manuelaristaran sugirió usar alguna técnica de Natural Language Processing](https://twitter.com/manuelaristaran/status/1249775645929934851) para esta tarea.  \n",
        "Una hipálage es una combinación de palabras en la que se le atribuye a un sustantivo una acción (verbo) o una cualidad (adjetivo) que son propios de otro. Por ejemplo, la expresión \"noche unámine\" en el Las Ruinas Circulares.  \n",
        "Siguiendo la sugerencia, decidimos abordar de forma exploratoria esta tarea con los siguientes pasos:  \n",
        "- Restringimos el análisis a pares de palabras sustantivo-adjetivo y a los cuentos de Ficciones **en inglés**\n",
        "- Extraemos todos los pares sustantivo-adjetivo de los cuentos usando un [modelo de dependecy parsing pre-entrenado](https://stanfordnlp.github.io/stanza/depparse.html) que reconoce las dependencias gramáticales en cada oración\n",
        "- Medimos las distancias coseno entre los [word embeddings GloVe](https://nlp.stanford.edu/projects/glove/) de cada par de palabras\n",
        "- Rankeamos los pares según la distancia coseno y exploramos un ranking alternativo que normaliza la distancia por la distancia promedio del sustantivo o adejtivo a los *n* adjetivos o sustantivos más cercanos.\n",
        "\n",
        "La hipótesis es que hay una mayor probabilidad de encontrar hipálages en pares sustatntivo-adjetivo con distancia alta entre sus embeddings que en pares con distancia baja. Esto se debe a que **en general** las palabras que conforman las hipálages tienen una similitud semántica baja o al menos no tienden a usarse en el mismo contexto. \n",
        "\n",
        "En este sentido, este análisis no nos permite estricamente encontrar hipálages (y aún menos evaluar la bondad de los resultados), sino más bien encontrar pares de sustantivo-adjetivos inusuales, llamativos o interesantes en relación al uso común de la lengua inglesa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78G5BTgWY-Hi",
        "colab_type": "text"
      },
      "source": [
        "### Descargas y librerías"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbBI9NqVR6_b",
        "colab_type": "text"
      },
      "source": [
        "- Instalamos [Stanza](https://github.com/stanfordnlp/stanza/), la librería desarrollada por Stanford que vamos a usar para encontrar las dependencias gramaticales en los textos. La librería ofrece modelos pre-entrenados para múltiples idiomas para aplicar en el contexto de tareas de NLP (en nuestro caso, dependency parsing). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43HvACxkRymt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hOjNVOosyyD",
        "colab_type": "text"
      },
      "source": [
        "- Descargamos `PyPDF2` para convertir *Ficciones* de formato PDF a un archivo de texto plano.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UneCaUCbsyGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install PyPDF2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWRwt3cfrH9o",
        "colab_type": "text"
      },
      "source": [
        "- Descargamos los modelos de `stanza` para textos en inglés. Los modelos se guardan por defecto en `/root/stanza_resources`. Esta carga nos llevó poco tiempo porque usamos Google Colab,  pero en otro contexto puede llevar algunos minutos -- lo mismo vale para el resto de instalaciones o descargas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG-14viUrFxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### PUEDE TARDAR VARIOS MINUTOS ###\n",
        "stanza.download('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f24WUOPTrehv",
        "colab_type": "text"
      },
      "source": [
        "- Cargamos las librerías que vamos a necesitar para hacer el análisis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMXXBGtUrgY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import stanza\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import gensim.downloader as gensim_api\n",
        "\n",
        "import PyPDF2 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU3cRBTTUm8U",
        "colab_type": "text"
      },
      "source": [
        "- Cargamos los vectores [GloVe](https://nlp.stanford.edu/pubs/glove.pdf), word embeddings ajustados por Stanford. En particular, vamos a usar los vectores de mayor dimensión posible (300) y ajustados con la wikipedia en inglés."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3xi5J58Ulyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_embeddings():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Returns: array of size (vocab_size, embeddings_dim)\n",
        "    \"\"\"\n",
        "    vectors = gensim_api.load(\"glove-wiki-gigaword-300\")\n",
        "    return vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mD_axisWcvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### PUEDE TARDAR VARIOS MINUTOS ###\n",
        "word_vectors = load_embeddings()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "602157AvS6wp",
        "colab_type": "text"
      },
      "source": [
        "- Descargamos los modelos de tokenización `punkt` porque son necesarios para separar el texto en oraciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l3TBrYtRGoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## revisar si esto es necesario\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Uj7VpRMca4g",
        "colab_type": "text"
      },
      "source": [
        "- Convertimos el pdf con los cuentos en un archivo de texto. Leemos este archivo y lo almacenamos en `texto` como una gran cadena de caracteres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tao4BqxXclFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ACLARAR CUANTO TARDA\n",
        "def pdf_to_txt(pdf_file, txt_file):\n",
        "    \"\"\" Convert pdf_file to txt_file with strings\n",
        "    \"\"\"\n",
        "    with open(pdf_file,'rb') as pdf, open(txt_file, 'w') as txt:\n",
        "        read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
        "        number_of_pages = read_pdf.getNumPages()\n",
        "        for page_number in range(number_of_pages):  \n",
        "            page = read_pdf.getPage(page_number)\n",
        "            page_content = page.extractText()\n",
        "            text_file.write(page_content)\n",
        "\n",
        "pdf_to_txt('borges_collected_fictions.pdf', 'ficciones.txt')\n",
        "with open('ficciones.txt', \"r\", encoding='utf-8') as f:\n",
        "    texto = f.read() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL4vFx33fvlT",
        "colab_type": "text"
      },
      "source": [
        "### Limpieza del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFDGTupPf2AK",
        "colab_type": "text"
      },
      "source": [
        "Limpiamos el texto de la siguiente manera:  \n",
        "- Reemplazamos los saltos de linea y cualquier otro carácter de tipo whitespace por espacios no repetidos porque no nos interesan los cambios de párrafo ni de página, sino solamente las oraciones  \n",
        "\n",
        "La documentación de `stanza` sugiere juntar varias oraciones en batches separando cada oración con dos saltos de linea `\\n\\n` para acelerar el procesamiento de muchos documentos. Sin embargo esto dificultaría rastrear al final del análisis las oraciones donde se encuentran las potenciales hipálages -- decidimos entonces conservar el texto como un solo string y que `stanza` se encargue de la tokenización.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Tz1tKKfybt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_texto(texto):\n",
        "    \"\"\"Cleans raw text\n",
        "       Returns: clean string\n",
        "    \"\"\"\n",
        "    clean_text = \" \".join(texto.split())\n",
        "    return clean_text\n",
        "\n",
        "texto = clean_texto(texto)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqCbHS-vtsE7",
        "colab_type": "text"
      },
      "source": [
        "Veamos los 300 caracteres del texto separados en oraciones para facilitar la visualización:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6Ot5QJvtvAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# REVISAR QUE FUNCIONE\n",
        "for s in sent_tokenize(texto[:300]):\n",
        "    print(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv3vYyauaApY",
        "colab_type": "text"
      },
      "source": [
        "### Dependencies parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gIezB-xaFCS",
        "colab_type": "text"
      },
      "source": [
        "Para realizar tareas de NLP con `stanza` es necesario construir un Pipeline con procesadores acordes al idioma de análisis.\n",
        "El Pipeline se inicializa por defecto con un dependency parser (`depparse`) para determinar las dependencias sintácticas entre las palabras de una oración dada. Para poder correr este modelo es necesario procesar el texto de antemano con los siguientes procesadores:\n",
        "- `tokenize`: separa el documento en oraciones\n",
        "- `pos`: identifica el rol de cada palabra en la oración con un modelo de Part-of-Speech (POS) tagging pre-entrenado\n",
        "- `lemma`: halla el lema correspondiente de cada palabra (la forma que representa las posibles formas flexionadas -- por ejemplo, \"walk\" es el lema de \"walking\")\n",
        "\n",
        "Por defecto estos procesadores se ejecutan antes de `depparse`, el cual usa la información generada en los pasos precedentes para identificar las relaciones sintátictas. Para identificar las relaciones sustantivo-adjetivo es necesario conservar las relaciones de tipo \"amod\" (*adjectival modifier*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-3vM_91aHFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = stanza.Pipeline('en', 'tokenize,pos,lemma,depparse', verbose=False)\n",
        "docs = nlp(texto)\n",
        "\n",
        "def find_dependencies(doc):\n",
        "    \"\"\"Find amod dependencies in one parsed doc with one or more sentences \n",
        "    Returns: list of dependencies with (sentence_idx, head, dependent)\n",
        "    \"\"\"\n",
        "    deps = list()\n",
        "    for sent_idx, sent in enumerate(doc.sentences):\n",
        "        id2word = {word.id: word.text for word in sent.words}\n",
        "        deps += [(sent_idx, id2word[str(word.head)], word.text) \\\n",
        "                for word in sent.words if word.deprel=='amod']\n",
        "    return deps\n",
        "    \n",
        "deps = find_dependencies(docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KL0xmf55A6ra"
      },
      "source": [
        "## Word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1mp5kzDAZaE",
        "colab_type": "text"
      },
      "source": [
        "Calculamos las distancias entre los embeddings de las palabras que forman la dependencia y luego generamos un DataFrame para guardarlas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K74AdSr1xJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = word_vectors.vocab.keys()\n",
        "deps_lists = [list(d) for d in deps]\n",
        "for dep in deps_lists:\n",
        "    _words = dep[1:3] \n",
        "    if set(_words).issubset(vocab):\n",
        "        dep.append(word_vectors.distance(*_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcvA4iaUmh9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dep = pd.DataFrame(deps_lists, columns =['sent_idx', 'head', 'dep', 'dist']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdVKomV_NNLI",
        "colab_type": "text"
      },
      "source": [
        "*Agrega* distancia con respecto a palabra más cercana\n",
        "(despues: modificar para que sea un adjetivo / sustantivo!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upsJREdXNkKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def most_similar(word, n):\n",
        "    \"\"\"Return nth most similar word and distance\"\"\"\n",
        "    if word in vocab:\n",
        "        out = word_vectors.most_similar(word, topn=20)[n+1]\n",
        "    else:\n",
        "        out = (np.nan, np.nan)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94pzUSi72uM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_upos(palabra):\n",
        "    doc = nlp(palabra)\n",
        "    upos = doc.sentences[0].words[0].upos\n",
        "    return upos\n",
        "def distance_most_similar_adjs(tmp, topn=5):\n",
        "    distancias = list()\n",
        "    k = 0\n",
        "    for i in tmp:\n",
        "        if get_upos(i[0]) == 'ADJ':\n",
        "            out.append(i[1])\n",
        "            k += 1\n",
        "        if k == topn:\n",
        "            break\n",
        "    avg_distance = sum(distancias) / len(distancias)\n",
        "    return avg_distance\n",
        "def distance_most_similar(tmp, type='ADJ', topn=5):\n",
        "    \"\"\"type 'ADJ' or 'NOUN'\n",
        "    \"\"\"\n",
        "    distancias = list()\n",
        "    k = 0\n",
        "    for i in tmp:\n",
        "        if get_upos(i[0]) == type:\n",
        "            out.append(i[1])\n",
        "            k += 1\n",
        "        if k == topn:\n",
        "            break\n",
        "    avg_distance = sum(distancias) / len(distancias)\n",
        "    return avg_distance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2R9jVrbNMrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dep[['dep_closest_word','dep_closest_dist']] = \\\n",
        "    data_dep.apply(lambda x: pd.Series(most_similar(x['dep'], n=1)), axis=1)\n",
        "data_dep[['head_closest_word','head_closest_dist']] = \\\n",
        "    data_dep.apply(lambda x: pd.Series(most_similar(x['head'], n=1)), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4m8jD_8PhjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dep['ratio_dep'] = data_dep['dist'] / data_dep['dep_closest_dist']\n",
        "data_dep['ratio_head'] = data_dep['dist'] / data_dep['head_closest_dist']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQYMS5hs5MeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# exploramos\n",
        "# data_dep.sort_values(by=['dist'], inplace=True, ascending=False)\n",
        "# data_dep.sort_values(by=['ratio_dep'], inplace=True, ascending=False)\n",
        "data_dep.sort_values(by=['ratio_head'], inplace=True, ascending=False)\n",
        "df_top = data_dep.head(20).copy()\n",
        "df_top['sentence'] = df_top['sent_idx'].apply(lambda x: docs_.sentences[x].text)\n",
        "df_bottom = data_dep.tail(20).copy()\n",
        "df_bottom['sentence'] = df_bottom['sent_idx'].apply(lambda x: docs_.sentences[x].text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqqlLZIFLtFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,j,k in zip(df_top['dep'], df_top['head'], df_top['sentence']):\n",
        "    print(i,j,' -- ',k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH-_1ofN7_zP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_bottom[['dep','head','dist']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhTiAjJDMBjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ojaldre con estas cosas:\n",
        "'Library' in vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1aYlzOa5tlC",
        "colab_type": "text"
      },
      "source": [
        "*Cosas viejas*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGFxb2yd67dM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lengths = [len(doc.sentences) for doc in docs]\n",
        "for i in lengths:\n",
        "    print(i)\n",
        "# No siempre da 1!!! (pero se puede corregir el idx de abajo creo)\n",
        "print(deps[len(deps)-1])\n",
        "sentences[129]\n",
        "print(deps[:5])\n",
        "# funciona el rastreo de oraciones!! :)\n",
        "\n",
        "# def words_distance(vocab, *words):\n",
        "#     \"\"\" Get (cosine?) distance between 2 Glove word vectors if in vocabulary \\ \n",
        "#         (dict_keys) \n",
        "#     Returns: distance (float)\n",
        "#     \"\"\"\n",
        "#     if set(words).issubset(vocab):\n",
        "#         d = word_vectors.distance(*words)\n",
        "#     else:\n",
        "#         d = np.nan\n",
        "#     return d\n",
        "# vocab = word_vectors.vocab.keys()\n",
        "# data_dep = pd.DataFrame(deps, columns =['sent_idx', 'head', 'dep']) \n",
        "# data_dep['dist'] = data_dep.apply(lambda x: \\\n",
        "#                                   words_distance(vocab, x['head'], x['dep']), \n",
        "#                                   axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt_nuLKj2JgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### OLD CODE\n",
        "\n",
        "# def find_dependencies_v2(docs):\n",
        "#     \"\"\"Find amod dependencies in many parsed docs (idx of sentence with \\\n",
        "#     respect to all docs)\n",
        "#     Returns: list of dependencies with (sentence_idx, head, dependent)\n",
        "#     \"\"\"\n",
        "#     deps = list()\n",
        "#     sent_idx = 0\n",
        "#     for doc in docs:\n",
        "#         for sent in doc.sentences:\n",
        "#             id2word = {word.id: word.text for word in sent.words}\n",
        "#             deps += [(sent_idx, id2word[str(word.head)], word.text) \\\n",
        "#                     for word in sent.words if word.deprel=='amod']\n",
        "#         sent_idx += 1\n",
        "#     return deps\n",
        "# deps = find_dependencies(docs)\n",
        "\n",
        "# def create_batches(sentences, sentences_per_doc=16):\n",
        "#     \"\"\"Creates batches of sentences\n",
        "#        Returns: list of strings, each one containg multiple sentences separated\n",
        "#        by \\n\\n \n",
        "#     \"\"\"\n",
        "#     ranges = [(max(0,i), min(i+sentences_per_doc,len(sentences))) \\\n",
        "#                  for i, x in enumerate(sentences) if \\\n",
        "#                  i % sentences_per_doc == 0]\n",
        "#     batches = ['\\n\\n'.join(sentences[i:s]) for i, s in ranges]\n",
        "#     return batches\n",
        "\n",
        "# sentences = clean_texto(texto)\n",
        "# batches = create_batches(sentences, sentences_per_doc=16)\n",
        "# print('{} sentences found'.format(len(sentences)))\n",
        "# print('{} batches created'.format(len(batches)))\n",
        "\n",
        "# def parse_sentences(batches):\n",
        "#     \"\"\"Parse batched sentences \n",
        "#     Returns: a list of docs, each one cointaining multiple parsed sentences\n",
        "#     \"\"\"\n",
        "#     docs = list()\n",
        "#     for doc in batches:\n",
        "#         docs.append(nlp(doc))\n",
        "#     # ponemos los docs en una sola lista\n",
        "#     docs = list(docs)\n",
        "#     return docs\n",
        "# # docs = parse_sentences(batches)\n",
        "# docs = parse_sentences(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}