{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "borges_hipalages.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ftvalentini/misc-notebooks/blob/master/borges_hipalages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsDGAS4lSigV",
        "colab_type": "text"
      },
      "source": [
        "# Buscando hipálages en los cuentos de Borges con NLP\n",
        "#### Autores: Julián Martínez Correa y Francisco Valentini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFRHqPQuV6V5",
        "colab_type": "text"
      },
      "source": [
        "En Twitter alguién sugirió recopilar todas las hipálages de la literatura de Borges y [@manuelaristaran sugirió usar alguna técnica de Natural Language Processing](https://twitter.com/manuelaristaran/status/1249775645929934851) para esta tarea.  \n",
        "Una hipálage es una combinación de palabras en la que se le atribuye a un sustantivo una acción (verbo) o una cualidad (adjetivo) que son propios de otro. Por ejemplo, la expresión \"noche unámine\" en el Las Ruinas Circulares.  \n",
        "Siguiendo la sugerencia, decidimos abordar de forma exploratoria esta tarea con los siguientes pasos:  \n",
        "- Restringimos el análisis a pares de palabras sustantivo-adjetivo y a los cuentos de Ficciones **en inglés**\n",
        "- Extraemos todos los pares sustantivo-adjetivo de los cuentos usando un [modelo de dependecy parsing pre-entrenado](https://stanfordnlp.github.io/stanza/depparse.html) que reconoce las dependencias gramáticales en cada oración\n",
        "- Medimos las distancias coseno entre los [word embeddings GloVe](https://nlp.stanford.edu/projects/glove/) de cada par de palabras. Un word embedding es un vector de dimensión *n* que codifica el signifcado de una palabra en relación al resto de las palabras de un vocabulario.\n",
        "- Rankeamos los pares según la distancia coseno y exploramos rankings alternativos en función de la inspección visual de los resultados. \n",
        "<!-- normaliza la distancia por la distancia promedio del sustantivo o adejtivo a los *n* adjetivos o sustantivos más cercanos. -->\n",
        "\n",
        "La hipótesis es que hay una mayor probabilidad de encontrar hipálages en pares sustantivo-adjetivo con distancia alta entre sus embeddings que en pares con distancia baja. Esto se debe a que **en general** las palabras que conforman las hipálages tienen una similitud semántica baja o al menos no tienden a usarse en el mismo contexto. \n",
        "\n",
        "En este sentido, este análisis no nos permite estricamente encontrar hipálages (y aún menos evaluar la bondad de los resultados), sino más bien encontrar pares de sustantivo-adjetivos inusuales, llamativos o interesantes en relación al uso común de la lengua inglesa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78G5BTgWY-Hi",
        "colab_type": "text"
      },
      "source": [
        "### Descargas y librerías"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbBI9NqVR6_b",
        "colab_type": "text"
      },
      "source": [
        "- Instalamos [Stanza](https://github.com/stanfordnlp/stanza/), la librería desarrollada por Stanford que vamos a usar para encontrar las dependencias gramaticales en los textos. La librería ofrece modelos pre-entrenados para múltiples idiomas para aplicar en el contexto de tareas de NLP (en nuestro caso, dependency parsing). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43HvACxkRymt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hOjNVOosyyD",
        "colab_type": "text"
      },
      "source": [
        "- Descargamos `PyPDF2` para convertir *Ficciones* de formato PDF a un archivo de texto plano.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UneCaUCbsyGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install PyPDF2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f24WUOPTrehv",
        "colab_type": "text"
      },
      "source": [
        "- Cargamos las librerías que vamos a necesitar para hacer el análisis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMXXBGtUrgY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import stanza\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import gensim.downloader as gensim_api\n",
        "\n",
        "import PyPDF2 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWRwt3cfrH9o",
        "colab_type": "text"
      },
      "source": [
        "- Descargamos los modelos de `stanza` para textos en inglés. Los modelos se guardan por defecto en `/root/stanza_resources`. Esta carga nos llevó poco tiempo porque usamos Google Colab,  pero en otro contexto puede llevar algunos minutos -- lo mismo vale para el resto de instalaciones o descargas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG-14viUrFxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### PUEDE TARDAR VARIOS MINUTOS ###\n",
        "stanza.download('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU3cRBTTUm8U",
        "colab_type": "text"
      },
      "source": [
        "- Cargamos los vectores [GloVe](https://nlp.stanford.edu/pubs/glove.pdf), word embeddings ajustados por Stanford. En particular, vamos a usar los vectores de mayor dimensión posible (300) y ajustados con la wikipedia en inglés."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3xi5J58Ulyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_embeddings():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Returns: array of size (vocab_size, embeddings_dim)\n",
        "    \"\"\"\n",
        "    vectors = gensim_api.load(\"glove-wiki-gigaword-300\")\n",
        "    return vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mD_axisWcvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### PUEDE TARDAR VARIOS MINUTOS ###\n",
        "word_vectors = load_embeddings()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "602157AvS6wp",
        "colab_type": "text"
      },
      "source": [
        "- Descargamos los modelos de tokenización `punkt` para separar textos en oraciones. Solo lo vamos a usar para visualizar el texto de forma más amigable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l3TBrYtRGoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## revisar si esto es necesario\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Uj7VpRMca4g",
        "colab_type": "text"
      },
      "source": [
        "- Convertimos el pdf con los cuentos en un archivo de texto. Leemos este archivo y lo almacenamos en `texto` como una gran cadena de caracteres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tao4BqxXclFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pdf_to_txt(pdf_file, txt_file, pdf_first_page=10):\n",
        "    \"\"\" Convert pdf_file to txt_file with strings\n",
        "    \"\"\"\n",
        "    with open(pdf_file,'rb') as pdf, open(txt_file, 'w') as txt:\n",
        "        read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
        "        number_of_pages = read_pdf.getNumPages()\n",
        "        for page_number in range(pdf_first_page-1, number_of_pages):  \n",
        "            page = read_pdf.getPage(page_number)\n",
        "            page_content = page.extractText()\n",
        "            txt.write(page_content)\n",
        "\n",
        "def read_text(file):\n",
        "    \"\"\" Reads file as string\n",
        "    file can be PDF or txt -- if it is PDF it first transforms to txt\n",
        "    \"\"\"\n",
        "    file_root = os.path.splitext(file)[0]   \n",
        "    file_ext = os.path.splitext(file)[1] \n",
        "    if file_ext == '.pdf':\n",
        "        pdf_to_txt(file, file_root+'.txt')\n",
        "    with open(file_root+'.txt', \"r\", encoding='utf-8') as f:\n",
        "        texto = f.read() \n",
        "    return texto"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ldCOx3Zs-FB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "676fbce3-952b-4b21-c1df-a30fad0c6ad3"
      },
      "source": [
        "%%time\n",
        "input_file = 'borges_collected-fictions.pdf'\n",
        "# input_file = 'the_library_of_babel.txt'\n",
        "texto = read_text(input_file)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6.99 s, sys: 17 ms, total: 7.01 s\n",
            "Wall time: 7.01 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL4vFx33fvlT",
        "colab_type": "text"
      },
      "source": [
        "### Limpieza del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFDGTupPf2AK",
        "colab_type": "text"
      },
      "source": [
        "Limpiamos el texto de la siguiente manera:  \n",
        "- Reemplazamos los saltos de linea y cualquier otro carácter de tipo whitespace por espacios no repetidos porque no nos interesan los cambios de párrafo ni de página, sino solamente las oraciones  \n",
        "- Reemplazamos el carácter Š por --\n",
        "\n",
        "La documentación de `stanza` sugiere juntar varias oraciones en batches separando cada oración con dos saltos de linea `\\n\\n` para acelerar el procesamiento de muchos documentos. Sin embargo esto dificultaría rastrear al final del análisis las oraciones donde se encuentran las potenciales hipálages -- decidimos entonces conservar el texto como un solo string y que `stanza` se encargue de la tokenización.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Tz1tKKfybt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_texto(texto):\n",
        "    \"\"\"Cleans raw text\n",
        "       Returns: clean string\n",
        "    \"\"\"\n",
        "    clean_text = \" \".join(texto.split())\n",
        "    clean_text = clean_text.replace(\"Š\",\"—\")\n",
        "    return clean_text\n",
        "\n",
        "texto = clean_texto(texto)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqCbHS-vtsE7",
        "colab_type": "text"
      },
      "source": [
        "Veamos los primeros 500 caracteres del texto separados en oraciones para facilitar la visualización:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6Ot5QJvtvAM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c3fea6ba-e84d-4bf2-b580-d49d4b59568c"
      },
      "source": [
        "for s in sent_tokenize(texto[:500]):\n",
        "    print(s)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A Universal History of Iniquity (1935) I inscribe this book to S.D.\n",
            "— English, innumerable, and an Angel.\n",
            "Also: I offer her that kernel of myself that I have saved, somehow— the central heart that deals not in words, traffics not with dreams, and is untouched by time, by joy, by adversities.\n",
            "Preface to the First Edition The exercises in narrative prose that constitute this book were performed from 1933 to 1934.\n",
            "They are derived, I think, from my re-readings of Stevenson and Chesterton, from the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv3vYyauaApY",
        "colab_type": "text"
      },
      "source": [
        "### Dependencies parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gIezB-xaFCS",
        "colab_type": "text"
      },
      "source": [
        "Para realizar tareas de NLP con `stanza` es necesario construir un Pipeline con procesadores acordes al idioma de análisis.\n",
        "El Pipeline se inicializa por defecto con un dependency parser (`depparse`) para determinar las dependencias sintácticas entre las palabras de una oración dada. Para poder correr este modelo es necesario procesar el texto de antemano con los siguientes procesadores:\n",
        "- `tokenize`: separa el documento en oraciones\n",
        "- `pos`: identifica el rol de cada palabra en la oración con un modelo de Part-of-Speech (POS) tagging pre-entrenado\n",
        "- `lemma`: halla el lema correspondiente de cada palabra (la forma que representa las posibles formas flexionadas -- por ejemplo, \"walk\" es el lema de \"walking\")\n",
        "\n",
        "Por defecto estos procesadores se ejecutan antes de `depparse`, el cual usa la información generada en los pasos precedentes para identificar las relaciones sintátictas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-3vM_91aHFe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9409a851-7b1c-40ad-80c3-2945ad29ce47"
      },
      "source": [
        "%%time\n",
        "nlp = stanza.Pipeline('en', verbose=False)\n",
        "docs = nlp(texto)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 37min 24s, sys: 20.4 s, total: 37min 45s\n",
            "Wall time: 37min 46s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYqXKSuOVB4i",
        "colab_type": "text"
      },
      "source": [
        "Para identificar las relaciones sustantivo-adjetivo es necesario conservar las relaciones de tipo \"amod\" (*adjectival modifier*). Para cada relación identificada conservamos el índice de la oración para poder rastrearla más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjMLY_v4T2D0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_dependencies(doc):\n",
        "    \"\"\"Find amod dependencies in one parsed doc with one or more sentences \n",
        "    Returns: list of dependencies with (sentence_idx, head, dependent)\n",
        "    \"\"\"\n",
        "    deps = list()\n",
        "    for sent_idx, sent in enumerate(doc.sentences):\n",
        "        id2word = {word.id: word.text for word in sent.words}\n",
        "        deps += [(sent_idx, id2word[str(word.head)], word.text) \\\n",
        "                for word in sent.words if word.deprel=='amod']\n",
        "    return deps\n",
        "    \n",
        "deps = find_dependencies(docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGKO8R24VXYr",
        "colab_type": "text"
      },
      "source": [
        "Veamos los tres primeros y los tres últimos pares hallados: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxrFcYe5VhEd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0ef2b734-fffb-43f0-a3e2-ea898d59cfd2"
      },
      "source": [
        "print(deps[:3])\n",
        "print(deps[-3:])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(2, 'heart', 'central'), (4, 'prose', 'narrative'), (5, 'films', 'first')]\n",
            "[(11110, 'layers', 'Everlasting'), (11115, 'aesthetics', 'literary'), (11115, 'others', 'many')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KL0xmf55A6ra"
      },
      "source": [
        "## Word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1mp5kzDAZaE",
        "colab_type": "text"
      },
      "source": [
        "Para cada par de palabras calculamos la distancia entre sus embeddings. Si no existe el word embedding para una palabra, no calculamos la distancia. Finalmente guardamos los resultados en un DataFrame. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K74AdSr1xJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# palabras disponibles en GloVe\n",
        "vocab = word_vectors.vocab.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEg_y5RKZZVD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "be8e6af5-e6c4-4f6a-817a-3ee156324799"
      },
      "source": [
        "%%time\n",
        "for i, dep in enumerate(deps):\n",
        "    _words = dep[1:] \n",
        "    if set(_words).issubset(vocab):\n",
        "        deps[i] = dep + (word_vectors.distance(*_words),)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`. [matutils.py:737]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9min 53s, sys: 3.74 s, total: 9min 57s\n",
            "Wall time: 9min 58s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcvA4iaUmh9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "3b2f0faf-6103-46f1-8719-f657ab6bdc52"
      },
      "source": [
        "deps_df = pd.DataFrame(deps, columns=['sent_idx', 'noun', 'adj', 'dist']) \n",
        "deps_df.sort_values(by=['dist'], inplace=True, ascending=False)\n",
        "deps_df.dropna(inplace=True)\n",
        "print(deps_df.shape)\n",
        "print(deps_df.head(20))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      sent_idx          noun           adj      dist\n",
            "4766      3853   schismatics           new  1.307657\n",
            "9161      8804  trivialities           new  1.283182\n",
            "891        651     anathemas         first  1.278724\n",
            "5877      4975     spiderweb        better  1.277311\n",
            "6931      5867      hecatomb        public  1.271595\n",
            "6244      5276        karats          good  1.262450\n",
            "4622      3721       paisano          same  1.247534\n",
            "6223      5259  alexandrines          long  1.247089\n",
            "3953      3152       impiety          near  1.246878\n",
            "1728      1386       milonga          same  1.242564\n",
            "7130      6084           men   punctilious  1.235335\n",
            "9750      9680        series  misfortunate  1.232602\n",
            "3815      2994          past    modifiable  1.231535\n",
            "649        441        battle    indiscreet  1.231510\n",
            "6350      5378        center     ineffable  1.231363\n",
            "7135      6086     forewords          long  1.224867\n",
            "2698      2058      hexagons        native  1.221834\n",
            "1165       937    plastering        former  1.221735\n",
            "8919      8364        hatpin        little  1.220267\n",
            "698        485         house   hunchbacked  1.218015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnjubIHuXXB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(deps_df.tail(20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdVKomV_NNLI",
        "colab_type": "text"
      },
      "source": [
        "*DESCRIBIR RESULTADOS*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGE_Hm8bVzLM",
        "colab_type": "text"
      },
      "source": [
        "Conservamos aproximadamente tres cuartos de los pares de palabras con distancia más alta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnU5Q7piTwyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deps_df = deps_df.head(7500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3bsmAtsWbee",
        "colab_type": "text"
      },
      "source": [
        "*describir: indicador normalizado por distancia a palabras mas cercanas para solucionar problemas del indicador anterior*\n",
        "*creamos funcion para extraer distancia promedio a palabras mas cercanas* \n",
        "*explicar brevemente lo de upos*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuWukJTPGthQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_upos(word):\n",
        "    \"\"\"Get Universal POS of a word by itself (without sentence context) \n",
        "    \"\"\"\n",
        "    doc = nlp(word)\n",
        "    upos = doc.sentences[0].words[0].upos\n",
        "    return upos\n",
        "\n",
        "def avg_distance_most_similar(word, tipo='ADJ', n=5, nsim=2000):\n",
        "    \"\"\"Return avg distance of a word to n most similar 'ADJ's or 'NOUN's\n",
        "    tipo in ('ADJ','NOUN','ALL')\n",
        "    \"\"\"\n",
        "    if word not in vocab:\n",
        "        avg_distance = np.nan\n",
        "        palabras = []\n",
        "    else:\n",
        "        most_similar = word_vectors.most_similar(word, topn=nsim)\n",
        "        distancias = list()\n",
        "        palabras = list()\n",
        "        k = 0\n",
        "        for i in most_similar:\n",
        "            if tipo in ('ADJ','NOUN'):\n",
        "                if get_upos(i[0]) == tipo:\n",
        "                    distancias.append(1 - (i[1]+1) / 2)\n",
        "                    palabras.append(i[0])\n",
        "                    k += 1\n",
        "            elif tipo == 'ALL':\n",
        "                distancias.append(1 - (i[1]+1) / 2)\n",
        "                palabras.append(i[0])\n",
        "                k += 1\n",
        "            if k == n:\n",
        "                break\n",
        "        avg_distance = sum(distancias) / len(distancias)\n",
        "    return palabras, avg_distance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR5gEUsuWZPG",
        "colab_type": "text"
      },
      "source": [
        "*explicar cada tipo de denominador*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPOYWr2fGtJr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7f95eeb9-5578-4683-9c33-829cb144d880"
      },
      "source": [
        "%%time\n",
        "# avg distance to adj/noun\n",
        "deps_df[['noun_closest_adjs','noun_avg_dist_adjs']] = \\\n",
        "    deps_df.apply(lambda x: pd.Series(avg_distance_most_similar(x['noun'], tipo='ADJ', n=5)), axis=1)\n",
        "deps_df[['adj_closest_nouns','adj_avg_dist_nouns']] = \\\n",
        "    deps_df.apply(lambda x: pd.Series(avg_distance_most_similar(x['adj'], tipo='NOUN', n=5)), axis=1)\n",
        "# avg distance to any word\n",
        "deps_df[['noun_closest_words','noun_avg_dist_words']] = \\\n",
        "    deps_df.apply(lambda x: pd.Series(avg_distance_most_similar(x['noun'], tipo='ALL', n=20)), axis=1)\n",
        "deps_df[['adj_closest_words','adj_avg_dist_words']] = \\\n",
        "    deps_df.apply(lambda x: pd.Series(avg_distance_most_similar(x['adj'], tipo='ALL', n=20)), axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`. [matutils.py:737]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_4QlCcMWfIz",
        "colab_type": "text"
      },
      "source": [
        "*4 rankings segun 4 denominadores --\n",
        "pensar como combinar los denominadores (media, max?)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUyLLnoUGs_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deps_df['ranking_noun_adjs'] = deps_df['dist'] / deps_df['noun_avg_dist_adjs']\n",
        "deps_df['ranking_adj_nouns'] = deps_df['dist'] / deps_df['adj_avg_dist_nouns']\n",
        "deps_df['ranking_noun_words'] = deps_df['dist'] / deps_df['noun_avg_dist_words']\n",
        "deps_df['ranking_adj_words'] = deps_df['dist'] / deps_df['adj_avg_dist_words']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgbxzWo9XOTU",
        "colab_type": "text"
      },
      "source": [
        "*al final de todo: recuperar las oraciones de los n mas altos*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQYMS5hs5MeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deps_df.sort_values(by=['ratio_head'], inplace=True, ascending=False)\n",
        "df_top = deps_df.head(20).copy()\n",
        "df_top['sentence'] = df_top['sent_idx'].apply(lambda x: docs.sentences[x].text)\n",
        "df_bottom = deps_df.tail(20).copy()\n",
        "df_bottom['sentence'] = df_bottom['sent_idx'].apply(lambda x: docs.sentences[x].text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqqlLZIFLtFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,j,k in zip(df_top['dep'], df_top['head'], df_top['sentence']):\n",
        "    print(i,j,' -- ',k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH-_1ofN7_zP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_bottom[['dep','head','dist']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1aYlzOa5tlC",
        "colab_type": "text"
      },
      "source": [
        "<!-- *Cosas viejas*\n",
        "\n",
        "\n",
        "\n",
        "lengths = [len(doc.sentences) for doc in docs]\n",
        "for i in lengths:\n",
        "    print(i)\n",
        "# No siempre da 1!!! (pero se puede corregir el idx de abajo creo)\n",
        "print(deps[len(deps)-1])\n",
        "sentences[129]\n",
        "print(deps[:5])\n",
        "# funciona el rastreo de oraciones!! :)\n",
        "\n",
        "# def words_distance(vocab, *words):\n",
        "#     \"\"\" Get (cosine?) distance between 2 Glove word vectors if in vocabulary \\ \n",
        "#         (dict_keys) \n",
        "#     Returns: distance (float)\n",
        "#     \"\"\"\n",
        "#     if set(words).issubset(vocab):\n",
        "#         d = word_vectors.distance(*words)\n",
        "#     else:\n",
        "#         d = np.nan\n",
        "#     return d\n",
        "# vocab = word_vectors.vocab.keys()\n",
        "# data_dep = pd.DataFrame(deps, columns =['sent_idx', 'head', 'dep']) \n",
        "# data_dep['dist'] = data_dep.apply(lambda x: \\\n",
        "#                                   words_distance(vocab, x['head'], x['dep']), \n",
        "#                                   axis=1)\n",
        "\n",
        "### OLD CODE\n",
        "\n",
        "# def find_dependencies_v2(docs):\n",
        "#     \"\"\"Find amod dependencies in many parsed docs (idx of sentence with \\\n",
        "#     respect to all docs)\n",
        "#     Returns: list of dependencies with (sentence_idx, head, dependent)\n",
        "#     \"\"\"\n",
        "#     deps = list()\n",
        "#     sent_idx = 0\n",
        "#     for doc in docs:\n",
        "#         for sent in doc.sentences:\n",
        "#             id2word = {word.id: word.text for word in sent.words}\n",
        "#             deps += [(sent_idx, id2word[str(word.head)], word.text) \\\n",
        "#                     for word in sent.words if word.deprel=='amod']\n",
        "#         sent_idx += 1\n",
        "#     return deps\n",
        "# deps = find_dependencies(docs)\n",
        "\n",
        "# def create_batches(sentences, sentences_per_doc=16):\n",
        "#     \"\"\"Creates batches of sentences\n",
        "#        Returns: list of strings, each one containg multiple sentences separated\n",
        "#        by \\n\\n \n",
        "#     \"\"\"\n",
        "#     ranges = [(max(0,i), min(i+sentences_per_doc,len(sentences))) \\\n",
        "#                  for i, x in enumerate(sentences) if \\\n",
        "#                  i % sentences_per_doc == 0]\n",
        "#     batches = ['\\n\\n'.join(sentences[i:s]) for i, s in ranges]\n",
        "#     return batches\n",
        "\n",
        "# sentences = clean_texto(texto)\n",
        "# batches = create_batches(sentences, sentences_per_doc=16)\n",
        "# print('{} sentences found'.format(len(sentences)))\n",
        "# print('{} batches created'.format(len(batches)))\n",
        "\n",
        "# def parse_sentences(batches):\n",
        "#     \"\"\"Parse batched sentences \n",
        "#     Returns: a list of docs, each one cointaining multiple parsed sentences\n",
        "#     \"\"\"\n",
        "#     docs = list()\n",
        "#     for doc in batches:\n",
        "#         docs.append(nlp(doc))\n",
        "#     # ponemos los docs en una sola lista\n",
        "#     docs = list(docs)\n",
        "#     return docs\n",
        "# # docs = parse_sentences(batches)\n",
        "# docs = parse_sentences(sentences) -->"
      ]
    }
  ]
}