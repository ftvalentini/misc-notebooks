{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "borges_hipalages.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMp/LM1DKmoYOm8NINCLSmG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ftvalentini/misc-notebooks/blob/master/borges_hipalages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsDGAS4lSigV",
        "colab_type": "text"
      },
      "source": [
        "# Buscando hipálages con NLP\n",
        "## Una aplicación con los cuentos de J.L. Borges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFRHqPQuV6V5",
        "colab_type": "text"
      },
      "source": [
        "**ACA VA UN RESUMEN**\n",
        "(usar GPU???)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78G5BTgWY-Hi",
        "colab_type": "text"
      },
      "source": [
        "### Descargas y librerías"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbBI9NqVR6_b",
        "colab_type": "text"
      },
      "source": [
        "1. Instalamos [Stanza](https://github.com/stanfordnlp/stanza/), la librería desarrollada por Stanford que vamos a usar para encontrar las dependencias gramaticales en los textos. La librería ofrece modelos pre-entrenados para múltiples idiomas para aplicar en el contexto de tareas de NLP (en nuestro caso, dependency parsing). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43HvACxkRymt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU3cRBTTUm8U",
        "colab_type": "text"
      },
      "source": [
        "2. Cargamos los vectores [GloVe](https://nlp.stanford.edu/pubs/glove.pdf), word embeddings ajustados por Stanford. En particular, vamos a usar los vectores de mayor dimensión posible (300) y ajustados con la wikipedia en inglés. ¡Atención que este paso puede demorar hasta 20 minutos! (En Google Colab no debería tardar más de 5 minutos)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3xi5J58Ulyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_embeddings():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Returns: array of size vocab_size, embeddings_dim)\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    vectors = api.load(\"glove-wiki-gigaword-300\")\n",
        "    return vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mD_axisWcvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### PUEDE TARDAR VARIOS MINUTOS ###\n",
        "word_vectors = load_embeddings()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "602157AvS6wp",
        "colab_type": "text"
      },
      "source": [
        "3. Cargamos las librerías que vamos a necesitar para hacer el análisis. También descargamos los modelos de tokenización `punkt` porque son necesarios para separar el texto en oraciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l3TBrYtRGoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import stanza\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XozYRjQ8YVA_",
        "colab_type": "text"
      },
      "source": [
        "4. Descargamos los modelos de `stanza` para textos en inglés. Los modelos se guardan por defecto en `/root/stanza_resources`. Este paso tarda no más de 5 minutos en Google Colab pero puede tardar más en otro entorno."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya7E_95jYVZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### PUEDE TARDAR VARIOS MINUTOS ###\n",
        "stanza.download('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Uj7VpRMca4g",
        "colab_type": "text"
      },
      "source": [
        "5. Levantamos el texto que vamos a analizar. En este caso, La Biblioteca de Babel, guardada en texto plano. Lo vamos a almacenar como una cadena de carácteres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tao4BqxXclFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txt_file = 'the_library_of_babel.txt'\n",
        "with open(txt_file, \"r\", encoding='utf-8') as f:\n",
        "    texto = f.read() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL4vFx33fvlT",
        "colab_type": "text"
      },
      "source": [
        "### Limpieza del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFDGTupPf2AK",
        "colab_type": "text"
      },
      "source": [
        "Limpiamos el texto siguiendo los siguientes pasos:  \n",
        "a. Reemplazamos los saltos de linea y cualquier otro carácter de tipo whitespace por espacios no repetidos, porque no nos interesan los cambios de párrafo ni de página, sino solamente las oraciones  \n",
        "b. Tokenizamos el texto en oraciones.  \n",
        "c. Juntamos varias oraciones en batches separando cada oración con dos saltos de linea `\\n\\n` -- de acuerdo a la documentación de `stanza` el procesamiento de muchos documentos es más veloz de esta manera. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Tz1tKKfybt",
        "colab_type": "code",
        "outputId": "fbbe6c41-5ce6-4e2e-ae7a-78fcf9dab1b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def clean_texto(texto):\n",
        "    \"\"\"Cleans and tokenizes raw text\n",
        "       Returns: list of clean sentences\n",
        "       by \\n\\n \n",
        "    \"\"\"\n",
        "    clean_text = \" \".join(texto.split())\n",
        "    sentences = sent_tokenize(clean_text, language='english')\n",
        "    return sentences\n",
        "\n",
        "def create_batches(sentences, sentences_per_doc=16):\n",
        "    \"\"\"Creates batches of sentences\n",
        "       Returns: list of strings, each one containg multiple sentences separated\n",
        "       by \\n\\n \n",
        "    \"\"\"\n",
        "    ranges = [(max(0,i), min(i+sentences_per_doc,len(sentences))) \\\n",
        "                 for i, x in enumerate(sentences) if \\\n",
        "                 i % sentences_per_doc == 0]\n",
        "    batches = ['\\n\\n'.join(sentences[i:s]) for i, s in ranges]\n",
        "    return batches\n",
        "\n",
        "sentences = clean_texto(texto)\n",
        "batches = create_batches(sentences, sentences_per_doc=16)\n",
        "print('{} sentences found'.format(len(sentences)))\n",
        "print('{} batches created'.format(len(batches)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "130 sentences found\n",
            "9 batches created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqCbHS-vtsE7",
        "colab_type": "text"
      },
      "source": [
        "Veamos las primeras oraciones del texto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6Ot5QJvtvAM",
        "colab_type": "code",
        "outputId": "fa32f839-b54b-4489-8bc0-2895161e4b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "for s in sentences[:5]:\n",
        "    print(s)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "By this art you may contemplate the variations of the 23 letters...\n",
            "The Anatomy of Melancholy, part 2, sect.\n",
            "II, mem.\n",
            "IV The universe (which others call the Library) is composed of an indefinite and perhaps infinite number of hexagonal galleries, with vast air shafts between, surrounded by very low railings.\n",
            "From any of the hexagons one can see, interminably, the upper and lower floors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv3vYyauaApY",
        "colab_type": "text"
      },
      "source": [
        "### Dependencies parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gIezB-xaFCS",
        "colab_type": "text"
      },
      "source": [
        "*HOLA*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-3vM_91aHFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = stanza.Pipeline('en', verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnTw7JqjC4CX",
        "colab_type": "text"
      },
      "source": [
        "*TAL CONVIENE USAR SENTENCES POR QUE DESPUES ES DIFICIL TRACKEAR LAS ORACIOENES ORIGINALES PORQUE NO HACE LOS CORTES EXACTAMENTE EN \\N\\N CUANDO SE USAN BATCHES*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lB-Ftf5uDor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_sentences(batches):\n",
        "    \"\"\"Parse batched sentences \n",
        "    Returns: a list of docs, each one cointaining multiple parsed sentences\n",
        "    \"\"\"\n",
        "    docs = list()\n",
        "    for doc in batches:\n",
        "        docs.append(nlp(doc))\n",
        "    # ponemos los docs en una sola lista\n",
        "    docs = list(docs)\n",
        "    return docs\n",
        "# docs = parse_sentences(batches)\n",
        "docs = parse_sentences(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTbYsq8PDL4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lengths = [len(doc.sentences) for doc in docs]\n",
        "for i in lengths:\n",
        "    print(i)\n",
        "# No siempre da 1!!! (pero se puede corregir el idx de abajo creo)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WQE8JkR2Dey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_dependencies(docs):\n",
        "    \"\"\"Find amod dependencies in parsed docs\n",
        "    Returns: list of dependencies with (sentence_idx, head, dependent)\n",
        "    \"\"\"\n",
        "    deps = list()\n",
        "    sent_idx = 0\n",
        "    for doc in docs:\n",
        "        for sent in doc.sentences:\n",
        "            id2word = {word.id: word.text for word in sent.words}\n",
        "            deps += [(sent_idx, id2word[str(word.head)], word.text) \\\n",
        "                    for word in sent.words if word.deprel=='amod']\n",
        "        sent_idx += 1\n",
        "    return deps\n",
        "deps = find_dependencies(docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWMGosypDpMR",
        "colab_type": "code",
        "outputId": "29db4a15-83bb-483d-855c-8f0976e501e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(deps[len(deps)-1])\n",
        "sentences[129]\n",
        "# funciona el rastreo de oraciones!! :)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(129, 'page', 'inconceivable')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The handling of this silky vade mecum would not be convenient: each apparent page would unfold into other analogous ones; the inconceivable middle page would have no reverse.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d0kPWxJ13lo",
        "colab_type": "text"
      },
      "source": [
        "## Word embeddings"
      ]
    }
  ]
}