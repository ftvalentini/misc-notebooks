---
title: "Sobre Gradient Boosting y Tumbas"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: sandstone
knit: (function(input, ...) {
  rmarkdown::render(input, output_dir="docs") 
  })
---

<!-- TODO: generalidades de aprendizaje supervisado (ver slides) -->
<!-- ## Aprendizaje supervisado -->

<!-- Función de pérdida $J(\mathbf{y},\mathbf{\hat{y}})$ -->

Boosting
--------

Las técnicas de ensamble estiman el target $y$ con una suma de $M$ estimadores base $h$ que dependen de $\mathbf{x} \in \mathbb{R}^p$ tal que para cada observación $i$:

$$\hat{y}_i = f(\mathbf{x}_i) = \sum_{m=1}^{M} h_m(\mathbf{x}_i)$$

En **boosting** en particular, los modelos $h_m$ son modelos "débiles" (**weak learners**) que se entrenan secuencialmente con versiones continuamente alteradas de los datos, de modo que la predicción general vaya mejorando lentamente en áreas donde no ajusta bien. Los modelos son débiles en el sentido en que son simples y reducen la pérdida $J$ en una magnitud levemente por encima de lo que lo haría una estimación al azar. Es necesario que sean poco complejos para evitar el sobreajuste.

Visto de otra forma, *boosting* resuelve el problema de optimización general

$$\min J(\mathbf{y},\mathbf{\hat{y}}) = J(\mathbf{y},\sum_{m=1}^{M} h_m(\mathbf{x}_i))$$

mediante una simplificación: en iteración $m$ se resuelve el problema

$$\min J(\mathbf{y},h_m(\mathbf{x}))$$

Es decir que en cada iteración se ajusta el estimador base $h_m$ y se agrega a la estimación general $f(\mathbf{x})$ tomando como dadas las estimaciones anteriores --los $h_m$ no se modifican una vez que se pasa a las iteraciones subsiguientes. Esta solución se conoce como **forward stagewise additive modeling**.

Gradient Boosting
-----------------

La técnica de *gradient boosting* ajusta la función $f$ usando **descenso por el gradiente en el espacio funcional**.

En el caso más tradicional de descenso por el gradiente, el descenso se hace en el espacio de los parámetros. Por ejemplo, si la predicción $\hat{\mathbf{y}}=f(\mathbf{x},\mathbf{\theta})$ dependiera de un vector de parámetros $\mathbf{\theta} \in \mathbb{R}^p$, se puede ajustar $f$ ajustando $\mathbf{\theta}$ secuencialmente desciendendo por el gradiente de la pérdida $J$ con respecto a $\theta$: 

$$ \mathbf{\theta}^{(m)} = \mathbf{\theta}^{(m-1)} - \eta \frac{\partial J}{\partial \mathbf{\theta}^{(m-1)}} $$

donde $\eta$ representa la tasa de aprendizaje --es decir, el tamaño del ajuste que se realiza en la dirección del gradiente-- y $\theta$ se inicializa en valores al azar. Desde luego, la forma específica que tome la derivada $\frac{\partial J}{\partial \mathbf{\theta}^{(m)}}$ dependerá de la función de pérdida elegida --que asumimos es diferenciable con respecto a $\mathbf{\theta}$-- y de la forma de $f(\mathbf{x},\mathbf{\theta})$. El vector $\mathbf{\theta}$ que se obtiene luego de las $M$ iteraciones puede usarse para predecir $\mathbf{y}$ cualquier set de observaciones. 

Es posible usar descenso por el gradiente en el espacio de los parámetros solamente para ciertas configuraciones del problema --por ejemplo, si $f$ fuera una proyección lineal $f(\mathbf{x},\mathbf{\theta})=\mathbf{x}^T\mathbf{\theta}$.

En algunos contextos esto no es posible y podemos recurrir al descenso por el gradiente en el espacio funcional: el objetivo $J$ se optimiza secuencialmente usando el gradiente de $J$ con respecto a las predicciones $\mathbf{\hat{y}}$ tal que:

$$ \mathbf{\hat{y}}^{(m)} = \mathbf{\hat{y}}^{(m-1)} - \eta \frac{\partial J}{\partial \mathbf{\hat{y}}^{(m-1)}} $$

donde $\mathbf{\hat{y}}$ se iniciliza en valores al azar.

Sin embargo esta forma de descender por el gradiente no devuelve ninguna forma funcional que se pueda aplicar a nuevas observaciones. Solo nos permite obtener predicciones $\hat{y}_i$ para las mismas observaciones $i$ con las que se realiza la optimización. De hecho, lo único que estamos haciendo es corregir $\mathbf{\hat{y}}$ iterativamente para que se acerque a $\mathbf{y}$, haciendo tender la pérdida a 0 en los datos de entrenamiento. 

Este problema se puede resolver de la siguiente manera:

En lugar de actualizar las predicciones con el verdadero valor del gradiente según las datos de entrenamiento, se puede entrenar un modelo en cada iteración que estime esta dirección. Estos modelos son los estimadores base $h_m$ de *boosting*. 

Definiendo $\mathbf{r}^{(m-1)}=\frac{\partial J}{\partial \mathbf{\hat{y}}^{(m-1)}}$ podemos redefinir la ecuación de ajuste como:

$$ \mathbf{\hat{y}}^{(m)} = \mathbf{\hat{y}}^{(m-1)} - \eta h_{m}(\mathbf{x}) $$

donde cada estimador $h_m$ de aprende un *mapping* de ${\mathbf{x}}$ hacia $\mathbf{r}^{(m-1)}$. La magnitud de las predicciones de $h_m$ en cada iteración dependerá entonces de la magnitud de los gradientes $\mathbf{r}^{(m-1)}$ durante el entrenamiento, que decrecen tendencialmente a medida que crece $m$. De hecho, los gradientes negativos pueden entenderse como **residuos generalizados** entre $\mathbf{y}$ y $\mathbf{\hat{y}}$ que aplican a cualquier función de pérdida.  

Este abordaje nos permite usar *boosting* con estimadores base CART y con cualquier función de pérdida diferenciable. En particular, podemos usar funciones de pérdida robustas a observaciones atípicas (como la *deviance* o *log-loss* en el contexto de clasificación, o *absolute loss* para regresión). La inducción de árboles es particularmente difícil para estas funciones de pérdida, y justamente la motivación del *gradient boosting* es superar esta dificultad.   
<!-- en particular lo que es dificil de resolver es (10.29) de ESL -->

Gradient Boosted Trees
----------------------

<!-- *incluir características de los árboles en la optimización -- ver Chen y ESL 10.9* -->

Extensiones
-----------

### Regularización

### Subsampling

Hiperparámetros
---------------

<!-- Dada una función de pérdida podemos definir los siguientes hiperparámetros de Gradient Boosted Trees hasta el momento: -->

<!-- - Número de iteraciones $M$ -->

<!-- - Tasa de aprendizaje $\eta$ -->

<!-- - """Tamaño""" de los árboles $XXX$ -->

<!-- - """Regularización""" -->

<!-- - """Subsampling""" -->


Implementaciones
----------------

### XGBoost


### LightGBM

Referencias
-----------

- https://web.stanford.edu/~hastie/ElemStatLearn/
- https://explained.ai/gradient-boosting/
- http://nicolas-hug.com/blog/gradient_boosting_descent
- https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf
