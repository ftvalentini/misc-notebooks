---
title: "Gradient boosting"
output: html_notebook
---

## Aprendizaje supervisado

## Boosting

Estimar $y$ con $\hat{y}$ con una suma de $K$ estimadores base $h_k$ que dependen de $\mathbf{x} \in \mathbb{R}^p$ tal que para cada observación $i$:

$$\hat{y}_i = f(\mathbf{x}_i) = \sum_{k=1}^{K} h_k(\mathbf{x}_i)$$

Los $h_k$ se entrenan secuencialmente usando la predicción del estimador anterior -- es decir, cada estimador $k$ se ajusta usando los residuos del estimador $k-1$. De esta manera, la función de predicción va mejorando lentamente en áreas donde no ajusta bien.

## Gradient Boosting

En particular, la técnica de *gradient boosting* ajusta la función $f$ usando descenso por el gradiente en el espacio funcional.

En el caso tradicional de descenso por el gradiente, el descenso se hace en el espacio de los parámetros. Por ejemplo, si la predicción $\hat{y}$ tuviera la forma de una proyección lineal que depende de un vector de parámetros $\mathbf{\theta} \in \mathbb{R}^p$ tal que $\hat{y}_i = f(\mathbf{x}_i) = \mathbf{x}_i^T\mathbf{\theta}$, se puede ajustar $f$ ajustando $\theta$ secuencialmente desciendendo por el gradiente de la pérdida $J$ con respecto a $\theta$: 

$$ \mathbf{\theta}^{(k)} = \mathbf{\theta}^{(k-1)} - \lambda \frac{\partial J}{\partial \mathbf{\theta}^{(k-1)}} $$
donde $\lambda$ representa la tasa de aprendizaje --es decir, el tamaño del ajuste que se realiza en la dirección del gradiente-- y $\theta$ se inicializa en valores al azar. Desde luego, la forma específica que tome la derivada $\frac{\partial J}{\partial \mathbf{\theta}^{(k)}}$ dependerá de la función de pérdida elegida. El vector $\theta$ que se obtiene luego de $K$ iteraciones puede usarse para predecir cualquier set de observaciones. 

En algunos modelos el vector de predicciones $\mathbf{\hat{y}}$ no depende de parámetros con respecto a los cuales se puedan tomar derivadas. Este el caso de los CART. En este contexto el objetivo $J$ se puede optimizar secuencialmente usando el gradiente de $J$ con respecto a las predicciones $\mathbf{\hat{y}}$ tal que:

$$ \mathbf{\hat{y}}^{(k)} = \mathbf{\hat{y}}^{(k-1)} - \lambda \frac{\partial J}{\partial \mathbf{\hat{y}}^{(k-1)}} $$

donde $\mathbf{\hat{y}}$ se iniciliza en valores al azar.

Sin embargo esta forma de descender por el gradiente no devuelve ninguna forma funcional que se pueda aplicar a nuevas observaciones. Solo nos permite obtener predicciones $\hat{y}_i$ para las mismas observaciones $i$ con las que se realiza la optimización. 

## Referencias

- https://explained.ai/gradient-boosting/
- https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf
- http://nicolas-hug.com/blog/gradient_boosting_descent
