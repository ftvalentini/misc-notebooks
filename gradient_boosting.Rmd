---
title: "Gradient boosting"
# output: html_notebook
output: 
  github_document: default
  html_notebook: default
---

## Aprendizaje supervisado

Función de pérdida $J(\mathbf{y},\mathbf{\hat{y}})$

## Boosting

*incluir cosas de Elements*

Estimar $y$ con $\hat{y}$ con una suma de $M$ estimadores base $h_m$ que dependen de $\mathbf{x} \in \mathbb{R}^p$ tal que para cada observación $i$:

$$\hat{y}_i = f(\mathbf{x}_i) = \sum_{m=1}^{M} h_m(\mathbf{x}_i)$$

Los $h_m$ se entrenan secuencialmente usando la predicción del estimador anterior -- es decir, cada estimador $m$ se ajusta usando los residuos del estimador $m-1$. De esta manera, la función de predicción va mejorando lentamente en áreas donde no ajusta bien.

## Gradient Boosting

En particular, la técnica de *gradient boosting* ajusta la función $f$ usando descenso por el gradiente en el espacio funcional.

En el caso tradicional de descenso por el gradiente, el descenso se hace en el espacio de los parámetros. Por ejemplo, si la predicción $\hat{y}$ tuviera la forma de una proyección lineal que depende de un vector de parámetros $\mathbf{\theta} \in \mathbb{R}^p$ tal que $\hat{y}_i = f(\mathbf{x}_i) = \mathbf{x}_i^T\mathbf{\theta}$, se puede ajustar $f$ ajustando $\theta$ secuencialmente desciendendo por el gradiente de la pérdida $J$ con respecto a $\theta$: 

$$ \mathbf{\theta}^{(m)} = \mathbf{\theta}^{(m-1)} - \eta \frac{\partial J}{\partial \mathbf{\theta}^{(m-1)}} $$

donde $\eta$ representa la tasa de aprendizaje --es decir, el tamaño del ajuste que se realiza en la dirección del gradiente-- y $\theta$ se inicializa en valores al azar. Desde luego, la forma específica que tome la derivada $\frac{\partial J}{\partial \mathbf{\theta}^{(m)}}$ dependerá de la función de pérdida elegida. El vector $\theta$ que se obtiene luego de $M$ iteraciones puede usarse para predecir cualquier set de observaciones. 

En algunos modelos el vector de predicciones $\mathbf{\hat{y}}$ no depende de parámetros con respecto a los cuales se puedan tomar derivadas. Este el caso de los CART. En este contexto el objetivo $J$ se puede optimizar secuencialmente usando el gradiente de $J$ con respecto a las predicciones $\mathbf{\hat{y}}$ tal que:

$$ \mathbf{\hat{y}}^{(m)} = \mathbf{\hat{y}}^{(m-1)} - \eta \frac{\partial J}{\partial \mathbf{\hat{y}}^{(m-1)}} $$

donde $\mathbf{\hat{y}}$ se iniciliza en valores al azar.

Sin embargo esta forma de descender por el gradiente no devuelve ninguna forma funcional que se pueda aplicar a nuevas observaciones. Solo nos permite obtener predicciones $\hat{y}_i$ para las mismas observaciones $i$ con las que se realiza la optimización. De hecho, lo único que estamos haciendo es corregir $\mathbf\hat{y}}$ iterativamente hasta que resulte ser igual a $\mathbf{y}$ y la pérdida sea igual a 0 en los datos de entrenamiento. 

Este problema se puede resolver de la siguiete manera:

En lugar de actualizar las predicciones con el verdadero valor del gradiente según las datos de entrenamiento, se puede entrenar un modelo en cada iteración que estime esta dirección. Estos modelos son los estimadores base $h_m$ de *boosting*. También se conocen como *weak learners* porque es necesario usar modelos poco complejos para evitar el sobreajuste. En el caso de *Gradient Boosted Trees* son CARTs.

Definiendo $r^{(m-1)}=\frac{\partial J}{\partial \mathbf{\hat{y}}^{(m-1)}}$ podemos redefinir la ecuación de ajuste como:

$$ \mathbf{\hat{y}}^{(m)} = \mathbf{\hat{y}}^{(m-1)} - \eta h_{m}(\mathbf{x}) $$

Cada estimador $h_m$ de la fórmula de predicción de boosting $\hat{y}_i = f(\mathbf{x}_i) = \sum_{m=1}^{M} h_m(\mathbf{x}_i)$ aprende un *mapping* de ${\mathbf{x}}$ hacia $r^{(m-1)}$. La magnitud de las predicciones de $h_m$ en cada iteración dependerá obviamente de la magnitud de los gradientes $r^{(m-1)}$ durante el entrenamiento, que decrecen tendencialmente a medida que crece $m$. De hecho, los gradientes pueden entenderse como residuos generalizados entre $\mathbf{y}$ y $\mathbf{\hat{y}}$ que aplican a cualquier función de pérdida.  

## Gradient Boosted Trees

*incluir características de los árboles en la optimización -- ver Chen y Elements*

## Extensiones

### Regularización

### Subsampling

## Hiperparámetros

Dada una función de pérdida podemos definir los siguientes hiperparámetros de Gradient Boosted Trees hasta el momento:

- Número de iteraciones $M$

- Tasa de aprendizaje $\eta$

- """Tamaño""" de los árboles $XXX$

- """Regularización"""

- """Subsampling"""


## Implementaciones


### XGBoost


### LightGBM

## Referencias

- https://web.stanford.edu/~hastie/ElemStatLearn/
- https://explained.ai/gradient-boosting/
- http://nicolas-hug.com/blog/gradient_boosting_descent
- https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf
